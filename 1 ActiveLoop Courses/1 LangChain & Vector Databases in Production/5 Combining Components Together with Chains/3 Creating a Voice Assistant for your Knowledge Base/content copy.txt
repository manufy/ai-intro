Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation How-to guides Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started How-to guides In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems: Repository How to create a repository on the Hub? How to configure it? How to interact with it? Download files How do I download a file from the Hub? How do I download a repository? Upload files How to upload a file or a folder? How to make changes to an existing repository on the Hub? Search How to efficiently search through the 200k+ public models, datasets and spaces? HfFileSystem How to interact with the Hub through a convenient interface that mimics Python's file interface? Inference How to make predictions using the accelerated Inference API? Community Tab How to interact with the Community tab (Discussions and Pull Requests)? Collections How to programmatically build collections? Cache How does the cache-system work? How to benefit from it? Model Cards How to create and share Model Cards? Manage your Space How to manage your Space hardware and configuration? Integrate a library What does it mean to integrate a library with the Hub? And how to do it? Webhooks server How to create a server to receive Webhooks and deploy it as a Space? < > Update on GitHub ‚ÜêInstallation Download files‚Üí How-to guides
Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Download files from the Hub Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Download files from the Hub The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to: Download and cache a single file. Download and cache an entire repository. Download files to a local folder. Download a single file The hf_hub_download() function is the main function for downloading files from the Hub. It downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path. The returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid having a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our caching guide. From latest version Select the file to download using the repo_id, repo_type and filename parameters. By default, the file will be considered as being part of a model repo. Copied >>> from huggingface_hub import hf_hub_download >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json") '/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json' # Download from a dataset >>> hf_hub_download(repo_id="google/fleurs", filename="fleurs.py", repo_type="dataset") '/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py' From specific version By default, the latest version from the main branch is downloaded. However, in some cases you want to download a file at a particular version (e.g. from a specific branch, a PR, a tag or a commit hash). To do so, use the revision parameter: Copied # Download from the `v1.0` tag >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="v1.0") # Download from the `test-branch` branch >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="test-branch") # Download from Pull Request #3 >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="refs/pr/3") # Download from a specific commit hash >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="877b84a8f93f2d619faa2a6e514a32beef88ab0a") Note: When using the commit hash, it must be the full-length hash instead of a 7-character commit hash. Construct a download URL In case you want to construct the URL used to download a file from a repo, you can use hf_hub_url() which returns a URL. Note that it is used internally by hf_hub_download(). Download an entire repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process. To download a whole repository, just pass the repo_id and repo_type: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp") '/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade' # Or from a dataset >>> snapshot_download(repo_id="google/fleurs", repo_type="dataset") '/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34' snapshot_download() downloads the latest revision by default. If you want a specific repository revision, use the revision parameter: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", revision="refs/pr/1") Filter files to download snapshot_download() provides an easy way to download a repository. However, you don‚Äôt always want to download the entire content of a repository. For example, you might want to prevent downloading all .bin files if you know you‚Äôll only use the .safetensors weights. You can do that using allow_patterns and ignore_patterns parameters. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. The pattern matching is based on fnmatch. For example, you can use allow_patterns to only download JSON configuration files: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", allow_patterns="*.json") On the other hand, ignore_patterns can exclude certain files from being downloaded. The following example ignores the .msgpack and .h5 file extensions: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", ignore_patterns=["*.msgpack", "*.h5"]) Finally, you can combine both to precisely filter your download. Here is an example to download all json and markdown files except vocab.json. Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="gpt2", allow_patterns=["*.md", "*.json"], ignore_patterns="vocab.json") Download file(s) to a local folder By default, we recommend using the cache system to download files from the Hub. You can specify a custom cache location using the cache_dir parameter in hf_hub_download() and snapshot_download(), or by setting the HF_HOME environment variable. However, if you need to download files to a specific folder, you can pass a local_dir parameter to the download function. This is useful to get a workflow closer to what the git command offers. The downloaded files will maintain their original file structure within the specified folder. For example, if filename="data/train.csv" and local_dir="path/to/folder", the resulting filepath will be "path/to/folder/data/train.csv". A ./huggingface/ folder is created at the root of your local directory containing metadata about the downloaded files. This prevents re-downloading files if they‚Äôre already up-to-date. If the metadata has changed, then the new file version is downloaded. This makes the local_dir optimized for pulling only the latest changes. After completing the download, you can safely remove the .huggingface/ folder if you no longer need it. However, be aware that re-running your script without this folder may result in longer recovery times, as metadata will be lost. Rest assured that your local data will remain intact and unaffected. Don‚Äôt worry about the .huggingface/ folder when committing changes to the Hub! This folder is automatically ignored by both git and upload_folder(). Download from the CLI You can use the huggingface-cli download command from the terminal to directly download files from the Hub. Internally, it uses the same hf_hub_download() and snapshot_download() helpers described above and prints the returned path to the terminal. Copied >>> huggingface-cli download gpt2 config.json /home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json You can download multiple files at once which displays a progress bar and returns the snapshot path in which the files are located: Copied >>> huggingface-cli download gpt2 config.json model.safetensors Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 23831.27it/s] /home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10 For more details about the CLI download command, please refer to the CLI guide. Faster downloads If you are running on a machine with high bandwidth, you can increase your download speed with hf_transfer, a Rust-based library developed to speed up file transfers with the Hub. To enable it: Specify the hf_transfer extra when installing huggingface_hub (e.g. pip install huggingface_hub[hf_transfer]). Set HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable. hf_transfer is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this section. < > Update on GitHub ‚ÜêOverview Upload files‚Üí Download files from the Hub Download a single file From latest version From specific version Construct a download URL Download an entire repository Filter files to download Download file(s) to a local folder Download from the CLI Faster downloads
Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Upload files to the Hub Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Upload files to the Hub Sharing your files and work is an important aspect of the Hub. The huggingface_hub offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files: without using Git. that are very large with Git LFS. with the commit context manager. with the push_to_hub() function. Whenever you want to upload files to the Hub, you need to log in to your Hugging Face account. For more details about authentication, check out this section. Upload a file Once you‚Äôve created a repository with create_repo(), you can upload a file to your repository using upload_file(). Specify the path of the file to upload, where you want to upload the file to in the repository, and the name of the repository you want to add the file to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> api.upload_file( ... path_or_fileobj="/path/to/local/folder/README.md", ... path_in_repo="README.md", ... repo_id="username/test-dataset", ... repo_type="dataset", ... ) Upload a folder Use the upload_folder() function to upload a local folder to an existing repository. Specify the path of the local folder to upload, where you want to upload the folder to in the repository, and the name of the repository you want to add the folder to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() # Upload all the content from the local folder to your remote Space. # By default, files are uploaded at the root of the repo >>> api.upload_folder( ... folder_path="/path/to/local/space", ... repo_id="username/my-cool-space", ... repo_type="space", ... ) By default, the .gitignore file will be taken into account to know which files should be committed or not. By default we check if a .gitignore file is present in a commit, and if not, we check if it exists on the Hub. Please be aware that only a .gitignore file present at the root of the directory with be used. We do not check for .gitignore files in subdirectories. If you don‚Äôt want to use an hardcoded .gitignore file, you can use the allow_patterns and ignore_patterns arguments to filter which files to upload. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. If both allow_patterns and ignore_patterns are provided, both constraints apply. Beside the .gitignore file and allow/ignore patterns, any .git/ folder present in any subdirectory will be ignored. Copied >>> api.upload_folder( ... folder_path="/path/to/local/folder", ... path_in_repo="my-dataset/train", # Upload to a specific folder ... repo_id="username/test-dataset", ... repo_type="dataset", ... ignore_patterns="**/logs/*.txt", # Ignore all text logs ... ) You can also use the delete_patterns argument to specify files you want to delete from the repo in the same commit. This can prove useful if you want to clean a remote folder before pushing files in it and you don‚Äôt know which files already exists. The example below uploads the local ./logs folder to the remote /experiment/logs/ folder. Only txt files are uploaded but before that, all previous logs on the repo on deleted. All of this in a single commit. Copied >>> api.upload_folder( ... folder_path="/path/to/local/folder/logs", ... repo_id="username/trained-model", ... path_in_repo="experiment/logs/", ... allow_patterns="*.txt", # Upload all local text files ... delete_patterns="*.txt", # Delete all remote text files before ... ) Upload from the CLI You can use the huggingface-cli upload command from the terminal to directly upload files to the Hub. Internally it uses the same upload_file() and upload_folder() helpers described above. You can either upload a single file or an entire folder: Copied # Usage: huggingface-cli upload [repo_id] [local_path] [path_in_repo] >>> huggingface-cli upload Wauplin/my-cool-model ./models/model.safetensors model.safetensors https://huggingface.co/Wauplin/my-cool-model/blob/main/model.safetensors >>> huggingface-cli upload Wauplin/my-cool-model ./models . https://huggingface.co/Wauplin/my-cool-model/tree/main local_path and path_in_repo are optional and can be implicitly inferred. If local_path is not set, the tool will check if a local folder or file has the same name as the repo_id. If that‚Äôs the case, its content will be uploaded. Otherwise, an exception is raised asking the user to explicitly set local_path. In any case, if path_in_repo is not set, files are uploaded at the root of the repo. For more details about the CLI upload command, please refer to the CLI guide. Advanced features In most cases, you won‚Äôt need more than upload_file() and upload_folder() to upload your files to the Hub. However, huggingface_hub has more advanced features to make things easier. Let‚Äôs have a look at them! Non-blocking uploads In some cases, you want to push data without blocking your main thread. This is particularly useful to upload logs and artifacts while continuing a training. To do so, you can use the run_as_future argument in both upload_file() and upload_folder(). This will return a concurrent.futures.Future object that you can use to check the status of the upload. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> future = api.upload_folder( # Upload in the background (non-blocking action) ... repo_id="username/my-model", ... folder_path="checkpoints-001", ... run_as_future=True, ... ) >>> future Future(...) >>> future.done() False >>> future.result() # Wait for the upload to complete (blocking action) ... Background jobs are queued when using run_as_future=True. This means that you are guaranteed that the jobs will be executed in the correct order. Even though background jobs are mostly useful to upload data/create commits, you can queue any method you like using run_as_future(). For instance, you can use it to create a repo and then upload data to it in the background. The built-in run_as_future argument in upload methods is just an alias around it. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> api.run_as_future(api.create_repo, "username/my-model", exists_ok=True) Future(...) >>> api.upload_file( ... repo_id="username/my-model", ... path_in_repo="file.txt", ... path_or_fileobj=b"file content", ... run_as_future=True, ... ) Future(...) Upload a folder by chunks upload_folder() makes it easy to upload an entire folder to the Hub. However, for large folders (thousands of files or hundreds of GB), it can still be challenging. If you have a folder with a lot of files, you might want to upload it in several commits. If you experience an error or a connection issue during the upload, you would not have to resume the process from the beginning. To upload a folder in multiple commits, just pass multi_commits=True as argument. Under the hood, huggingface_hub will list the files to upload/delete and split them in several commits. The ‚Äústrategy‚Äù (i.e. how to split the commits) is based on the number and size of the files to upload. A PR is open on the Hub to push all the commits. Once the PR is ready, the commits are squashed into a single commit. If the process is interrupted before completing, you can rerun your script to resume the upload. The created PR will be automatically detected and the upload will resume from where it stopped. It is recommended to pass multi_commits_verbose=True to get a better understanding of the upload and its progress. The example below will upload the checkpoints folder to a dataset in multiple commits. A PR will be created on the Hub and merged automatically once the upload is complete. If you prefer the PR to stay open and review it manually, you can pass create_pr=True. Copied >>> upload_folder( ... folder_path="local/checkpoints", ... repo_id="username/my-dataset", ... repo_type="dataset", ... multi_commits=True, ... multi_commits_verbose=True, ... ) If you want a better control on the upload strategy (i.e. the commits that are created), you can have a look at the low-level plan_multi_commits() and create_commits_on_pr() methods. multi_commits is still an experimental feature. Its API and behavior is subject to change in the future without prior notice. Scheduled uploads The Hugging Face Hub makes it easy to save and version data. However, there are some limitations when updating the same file thousands of times. For instance, you might want to save logs of a training process or user feedback on a deployed Space. In these cases, uploading the data as a dataset on the Hub makes sense, but it can be hard to do properly. The main reason is that you don‚Äôt want to version every update of your data because it‚Äôll make the git repository unusable. The CommitScheduler class offers a solution to this problem. The idea is to run a background job that regularly pushes a local folder to the Hub. Let‚Äôs assume you have a Gradio Space that takes as input some text and generates two translations of it. Then, the user can select their preferred translation. For each run, you want to save the input, output, and user preference to analyze the results. This is a perfect use case for CommitScheduler; you want to save data to the Hub (potentially millions of user feedback), but you don‚Äôt need to save in real-time each user‚Äôs input. Instead, you can save the data locally in a JSON file and upload it every 10 minutes. For example: Copied >>> import json >>> import uuid >>> from pathlib import Path >>> import gradio as gr >>> from huggingface_hub import CommitScheduler # Define the file where to save the data. Use UUID to make sure not to overwrite existing data from a previous run. >>> feedback_file = Path("user_feedback/") / f"data_{uuid.uuid4()}.json" >>> feedback_folder = feedback_file.parent # Schedule regular uploads. Remote repo and local folder are created if they don't already exist. >>> scheduler = CommitScheduler( ... repo_id="report-translation-feedback", ... repo_type="dataset", ... folder_path=feedback_folder, ... path_in_repo="data", ... every=10, ... ) # Define the function that will be called when the user submits its feedback (to be called in Gradio) >>> def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None: ... """ ... Append input/outputs and user feedback to a JSON Lines file using a thread lock to avoid concurrent writes from different users. ... """ ... with scheduler.lock: ... with feedback_file.open("a") as f: ... f.write(json.dumps({"input": input_text, "output_1": output_1, "output_2": output_2, "user_choice": user_choice})) ... f.write("\n") # Start Gradio >>> with gr.Blocks() as demo: >>> ... # define Gradio demo + use `save_feedback` >>> demo.launch() And that‚Äôs it! User input/outputs and feedback will be available as a dataset on the Hub. By using a unique JSON file name, you are guaranteed you won‚Äôt overwrite data from a previous run or data from another Spaces/replicas pushing concurrently to the same repository. For more details about the CommitScheduler, here is what you need to know: append-only: It is assumed that you will only add content to the folder. You must only append data to existing files or create new files. Deleting or overwriting a file might corrupt your repository. git history: The scheduler will commit the folder every every minutes. To avoid polluting the git repository too much, it is recommended to set a minimal value of 5 minutes. Besides, the scheduler is designed to avoid empty commits. If no new content is detected in the folder, the scheduled commit is dropped. errors: The scheduler run as background thread. It is started when you instantiate the class and never stops. In particular, if an error occurs during the upload (example: connection issue), the scheduler will silently ignore it and retry at the next scheduled commit. thread-safety: In most cases it is safe to assume that you can write to a file without having to worry about a lock file. The scheduler will not crash or be corrupted if you write content to the folder while it‚Äôs uploading. In practice, it is possible that concurrency issues happen for heavy-loaded apps. In this case, we advice to use the scheduler.lock lock to ensure thread-safety. The lock is blocked only when the scheduler scans the folder for changes, not when it uploads data. You can safely assume that it will not affect the user experience on your Space. Space persistence demo Persisting data from a Space to a Dataset on the Hub is the main use case for CommitScheduler. Depending on the use case, you might want to structure your data differently. The structure has to be robust to concurrent users and restarts which often implies generating UUIDs. Besides robustness, you should upload data in a format readable by the ü§ó Datasets library for later reuse. We created a Space that demonstrates how to save several different data formats (you may need to adapt it for your own specific needs). Custom uploads CommitScheduler assumes your data is append-only and should be uploading ‚Äúas is‚Äù. However, you might want to customize the way data is uploaded. You can do that by creating a class inheriting from CommitScheduler and overwrite the push_to_hub method (feel free to overwrite it any way you want). You are guaranteed it will be called every every minutes in a background thread. You don‚Äôt have to worry about concurrency and errors but you must be careful about other aspects, such as pushing empty commits or duplicated data. In the (simplified) example below, we overwrite push_to_hub to zip all PNG files in a single archive to avoid overloading the repo on the Hub: Copied class ZipScheduler(CommitScheduler): def push_to_hub(self): # 1. List PNG files png_files = list(self.folder_path.glob("*.png")) if len(png_files) == 0: return None # return early if nothing to commit # 2. Zip png files in a single archive with tempfile.TemporaryDirectory() as tmpdir: archive_path = Path(tmpdir) / "train.zip" with zipfile.ZipFile(archive_path, "w", zipfile.ZIP_DEFLATED) as zip: for png_file in png_files: zip.write(filename=png_file, arcname=png_file.name) # 3. Upload archive self.api.upload_file(..., path_or_fileobj=archive_path) # 4. Delete local png files to avoid re-uploading them later for png_file in png_files: png_file.unlink() When you overwrite push_to_hub, you have access to the attributes of CommitScheduler and especially: HfApi client: api Folder parameters: folder_path and path_in_repo Repo parameters: repo_id, repo_type, revision The thread lock: lock For more examples of custom schedulers, check out our demo Space containing different implementations depending on your use cases. create_commit The upload_file() and upload_folder() functions are high-level APIs that are generally convenient to use. We recommend trying these functions first if you don‚Äôt need to work at a lower level. However, if you want to work at a commit-level, you can use the create_commit() function directly. There are three types of operations supported by create_commit(): CommitOperationAdd uploads a file to the Hub. If the file already exists, the file contents are overwritten. This operation accepts two arguments: path_in_repo: the repository path to upload a file to. path_or_fileobj: either a path to a file on your filesystem or a file-like object. This is the content of the file to upload to the Hub. CommitOperationDelete removes a file or a folder from a repository. This operation accepts path_in_repo as an argument. CommitOperationCopy copies a file within a repository. This operation accepts three arguments: src_path_in_repo: the repository path of the file to copy. path_in_repo: the repository path where the file should be copied. src_revision: optional - the revision of the file to copy if your want to copy a file from a different branch/revision. For example, if you want to upload two files and delete a file in a Hub repository: Use the appropriate CommitOperation to add or delete a file and to delete a folder: Copied >>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete >>> api = HfApi() >>> operations = [ ... CommitOperationAdd(path_in_repo="LICENSE.md", path_or_fileobj="~/repo/LICENSE.md"), ... CommitOperationAdd(path_in_repo="weights.h5", path_or_fileobj="~/repo/weights-final.h5"), ... CommitOperationDelete(path_in_repo="old-weights.h5"), ... CommitOperationDelete(path_in_repo="logs/"), ... CommitOperationCopy(src_path_in_repo="image.png", path_in_repo="duplicate_image.png"), ... ] Pass your operations to create_commit(): Copied >>> api.create_commit( ... repo_id="lysandre/test-model", ... operations=operations, ... commit_message="Upload my model weights and license", ... ) In addition to upload_file() and upload_folder(), the following functions also use create_commit() under the hood: delete_file() deletes a single file from a repository on the Hub. delete_folder() deletes an entire folder from a repository on the Hub. metadata_update() updates a repository‚Äôs metadata. For more detailed information, take a look at the HfApi reference. Preupload LFS files before commit In some cases, you might want to upload huge files to S3 before making the commit call. For example, if you are committing a dataset in several shards that are generated in-memory, you would need to upload the shards one by one to avoid an out-of-memory issue. A solution is to upload each shard as a separate commit on the repo. While being perfectly valid, this solution has the drawback of potentially messing the git history by generating tens of commits. To overcome this issue, you can upload your files one by one to S3 and then create a single commit at the end. This is possible using preupload_lfs_files() in combination with create_commit(). This is a power-user method. Directly using upload_file(), upload_folder() or create_commit() instead of handling the low-level logic of pre-uploading files is the way to go in the vast majority of cases. The main caveat of preupload_lfs_files() is that until the commit is actually made, the upload files are not accessible on the repo on the Hub. If you have a question, feel free to ping us on our Discord or in a GitHub issue. Here is a simple example illustrating how to pre-upload files: Copied >>> from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo >>> repo_id = create_repo("test_preupload").repo_id >>> operations = [] # List of all `CommitOperationAdd` objects that will be generated >>> for i in range(5): ... content = ... # generate binary content ... addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content) ... preupload_lfs_files(repo_id, additions=[addition]) ... operations.append(addition) >>> # Create commit >>> create_commit(repo_id, operations=operations, commit_message="Commit all shards") First, we create the CommitOperationAdd objects one by one. In a real-world example, those would contain the generated shards. Each file is uploaded before generating the next one. During the preupload_lfs_files() step, the CommitOperationAdd object is mutated. You should only use it to pass it directly to create_commit(). The main update of the object is that the binary content is removed from it, meaning that it will be garbage-collected if you don‚Äôt store another reference to it. This is expected as we don‚Äôt want to keep in memory the content that is already uploaded. Finally we create the commit by passing all the operations to create_commit(). You can pass additional operations (add, delete or copy) that have not been processed yet and they will be handled correctly. Tips and tricks for large uploads There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data, getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying. Check out our Repository limitations and recommendations guide for best practices on how to structure your repositories on the Hub. Next, let‚Äôs move on with some practical tips to make your upload process as smooth as possible. Start small: We recommend starting with a small amount of data to test your upload script. It‚Äôs easier to iterate on a script when failing takes only a little time. Expect failures: Streaming large amounts of data is challenging. You don‚Äôt know what can happen, but it‚Äôs always best to consider that something will fail at least once -no matter if it‚Äôs due to your machine, your connection, or our servers. For example, if you plan to upload a large number of files, it‚Äôs best to keep track locally of which files you already uploaded before uploading the next batch. You are ensured that an LFS file that is already committed will never be re-uploaded twice but checking it client-side can still save some time. Use hf_transfer: this is a Rust-based library meant to speed up uploads on machines with very high bandwidth. To use hf_transfer: Specify the hf_transfer extra when installing huggingface_hub (e.g. pip install huggingface_hub[hf_transfer]). Set HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable. hf_transfer is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this section. (legacy) Upload files with Git LFS All the methods described above use the Hub‚Äôs API to upload files. This is the recommended way to upload files to the Hub. However, we also provide Repository, a wrapper around the git tool to manage a local repository. Although Repository is not formally deprecated, we recommend using the HTTP-based methods described above instead. For more details about this recommendation, please have a look at this guide explaining the core differences between HTTP-based and Git-based approaches. Git LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS: Copied huggingface-cli lfs-enable-largefiles You should install this for each repository that has a very large file. Once installed, you‚Äôll be able to push files larger than 5GB. commit context manager The commit context manager handles four of the most common Git commands: pull, add, commit, and push. git-lfs automatically tracks any file larger than 10MB. In the following example, the commit context manager: Pulls from the text-files repository. Adds a change made to file.txt. Commits the change. Pushes the change to the text-files repository. Copied >>> from huggingface_hub import Repository >>> with Repository(local_dir="text-files", clone_from="<user>/text-files").commit(commit_message="My first file :)"): ... with open("file.txt", "w+") as f: ... f.write(json.dumps({"hey": 8})) Here is another example of how to use the commit context manager to save and upload a file to a repository: Copied >>> import torch >>> model = torch.nn.Transformer() >>> with Repository("torch-model", clone_from="<user>/torch-model", token=True).commit(commit_message="My cool model :)"): ... torch.save(model.state_dict(), "model.pt") Set blocking=False if you would like to push your commits asynchronously. Non-blocking behavior is helpful when you want to continue running your script while your commits are being pushed. Copied >>> with repo.commit(commit_message="My cool model :)", blocking=False) You can check the status of your push with the command_queue method: Copied >>> last_command = repo.command_queue[-1] >>> last_command.status Refer to the table below for the possible statuses: Status Description -1 The push is ongoing. 0 The push has completed successfully. Non-zero An error has occurred. When blocking=False, commands are tracked, and your script will only exit when all pushes are completed, even if other errors occur in your script. Some additional useful commands for checking the status of a push include: Copied # Inspect an error. >>> last_command.stderr # Check whether a push is completed or ongoing. >>> last_command.is_done # Check whether a push command has errored. >>> last_command.failed push_to_hub The Repository class has a push_to_hub() function to add files, make a commit, and push them to a repository. Unlike the commit context manager, you‚Äôll need to pull from a repository first before calling push_to_hub(). For example, if you‚Äôve already cloned a repository from the Hub, then you can initialize the repo from the local directory: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="path/to/local/repo") Update your local clone with git_pull() and then push your file to the Hub: Copied >>> repo.git_pull() >>> repo.push_to_hub(commit_message="Commit my-awesome-file to the Hub") However, if you aren‚Äôt ready to push a file yet, you can use git_add() and git_commit() to only add and commit your file: Copied >>> repo.git_add("path/to/file") >>> repo.git_commit(commit_message="add my first model config file :)") When you‚Äôre ready, push the file to your repository with git_push(): Copied >>> repo.git_push() < > Update on GitHub ‚ÜêDownload files Use the CLI‚Üí Upload files to the Hub Upload a file Upload a folder Upload from the CLI Advanced features Non-blocking uploads Upload a folder by chunks Scheduled uploads Space persistence demo Custom uploads create_commit Preupload LFS files before commit Tips and tricks for large uploads (legacy) Upload files with Git LFS commit context manager push_to_hub
Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Interact with the Hub through the Filesystem API Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Interact with the Hub through the Filesystem API In addition to the HfApi, the huggingface_hub library provides HfFileSystem, a pythonic fsspec-compatible file interface to the Hugging Face Hub. The HfFileSystem builds of top of the HfApi and offers typical filesystem style operations like cp, mv, ls, du, glob, get_file, and put_file. Usage Copied >>> from huggingface_hub import HfFileSystem >>> fs = HfFileSystem() >>> # List all files in a directory >>> fs.ls("datasets/my-username/my-dataset-repo/data", detail=False) ['datasets/my-username/my-dataset-repo/data/train.csv', 'datasets/my-username/my-dataset-repo/data/test.csv'] >>> # List all ".csv" files in a repo >>> fs.glob("datasets/my-username/my-dataset-repo/**.csv") ['datasets/my-username/my-dataset-repo/data/train.csv', 'datasets/my-username/my-dataset-repo/data/test.csv'] >>> # Read a remote file >>> with fs.open("datasets/my-username/my-dataset-repo/data/train.csv", "r") as f: ... train_data = f.readlines() >>> # Read the content of a remote file as a string >>> train_data = fs.read_text("datasets/my-username/my-dataset-repo/data/train.csv", revision="dev") >>> # Write a remote file >>> with fs.open("datasets/my-username/my-dataset-repo/data/validation.csv", "w") as f: ... f.write("text,label") ... f.write("Fantastic movie!,good") The optional revision argument can be passed to run an operation from a specific commit such as a branch, tag name, or a commit hash. Unlike Python‚Äôs built-in open, fsspec‚Äôs open defaults to binary mode, "rb". This means you must explicitly set mode as "r" for reading and "w" for writing in text mode. Appending to a file (modes "a" and "ab") is not supported yet. Integrations The HfFileSystem can be used with any library that integrates fsspec, provided the URL follows the scheme: Copied hf://[<repo_type_prefix>]<repo_id>[@<revision>]/<path/in/repo> The repo_type_prefix is datasets/ for datasets, spaces/ for spaces, and models don‚Äôt need a prefix in the URL. Some interesting integrations where HfFileSystem simplifies interacting with the Hub are listed below: Reading/writing a Pandas DataFrame from/to a Hub repository: Copied >>> import pandas as pd >>> # Read a remote CSV file into a dataframe >>> df = pd.read_csv("hf://datasets/my-username/my-dataset-repo/train.csv") >>> # Write a dataframe to a remote CSV file >>> df.to_csv("hf://datasets/my-username/my-dataset-repo/test.csv") The same workflow can also be used for Dask and Polars DataFrames. Querying (remote) Hub files with DuckDB: Copied >>> from huggingface_hub import HfFileSystem >>> import duckdb >>> fs = HfFileSystem() >>> duckdb.register_filesystem(fs) >>> # Query a remote file and get the result back as a dataframe >>> fs_query_file = "hf://datasets/my-username/my-dataset-repo/data_dir/data.parquet" >>> df = duckdb.query(f"SELECT * FROM '{fs_query_file}' LIMIT 10").df() Using the Hub as an array store with Zarr: Copied >>> import numpy as np >>> import zarr >>> embeddings = np.random.randn(50000, 1000).astype("float32") >>> # Write an array to a repo >>> with zarr.open_group("hf://my-username/my-model-repo/array-store", mode="w") as root: ... foo = root.create_group("embeddings") ... foobar = foo.zeros('experiment_0', shape=(50000, 1000), chunks=(10000, 1000), dtype='f4') ... foobar[:] = embeddings >>> # Read an array from a repo >>> with zarr.open_group("hf://my-username/my-model-repo/array-store", mode="r") as root: ... first_row = root["embeddings/experiment_0"][0] Authentication In many cases, you must be logged in with a Hugging Face account to interact with the Hub. Refer to the Authentication section of the documentation to learn more about authentication methods on the Hub. It is also possible to login programmatically by passing your token as an argument to HfFileSystem: Copied >>> from huggingface_hub import HfFileSystem >>> fs = HfFileSystem(token=token) If you login this way, be careful not to accidentally leak the token when sharing your source code! < > Update on GitHub ‚ÜêUse the CLI Repository‚Üí Interact with the Hub through the Filesystem API Usage Integrations Authentication
Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Create and manage a repository Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Create and manage a repository The Hugging Face Hub is a collection of git repositories. Git is a widely used tool in software development to easily version projects when working collaboratively. This guide will show you how to interact with the repositories on the Hub, especially: Create and delete a repository. Manage branches and tags. Rename your repository. Update your repository visibility. Manage a local copy of your repository. If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct might be to use git CLI to clone your repo (git clone), commit changes (git add, git commit) and push them (git push). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do not share the same requirements and workflows. Model repositories might maintain large model weight files for different frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As a result, it may be more efficient to use our custom HTTP methods. You can read our Git vs HTTP paradigm explanation page for more details. If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to this section. In the rest of this guide, we will assume that your machine is logged in. Repo creation and deletion The first step is to know how to create and delete repositories. You can only manage repositories that you own (under your username namespace) or from organizations in which you have write permissions. Create a repository Create an empty repository with create_repo() and give it a name with the repo_id parameter. The repo_id is your namespace followed by the repository name: username_or_org/repo_name. Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-model") 'https://huggingface.co/lysandre/test-model' By default, create_repo() creates a model repository. But you can use the repo_type parameter to specify another repository type. For example, if you want to create a dataset repository: Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-dataset", repo_type="dataset") 'https://huggingface.co/datasets/lysandre/test-dataset' When you create a repository, you can set your repository visibility with the private parameter. Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-private", private=True) If you want to change the repository visibility at a later time, you can use the update_repo_visibility() function. Delete a repository Delete a repository with delete_repo(). Make sure you want to delete a repository because this is an irreversible process! Specify the repo_id of the repository you want to delete: Copied >>> delete_repo(repo_id="lysandre/my-corrupted-dataset", repo_type="dataset") Duplicate a repository (only for Spaces) In some cases, you want to copy someone else‚Äôs repo to adapt it to your use case. This is possible for Spaces using the duplicate_space() method. It will duplicate the whole repository. You will still need to configure your own settings (hardware, sleep-time, storage, variables and secrets). Check out our Manage your Space guide for more details. Copied >>> from huggingface_hub import duplicate_space >>> duplicate_space("multimodalart/dreambooth-training", private=False) RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...) Upload and download files Now that you have created your repository, you are interested in pushing changes to it and downloading files from it. These 2 topics deserve their own guides. Please refer to the upload and the download guides to learn how to use your repository. Branches and tags Git repositories often make use of branches to store different versions of a same repository. Tags can also be used to flag a specific state of your repository, for example, when releasing a version. More generally, branches and tags are referred as git references. Create branches and tags You can create new branch and tags using create_branch() and create_tag(): Copied >>> from huggingface_hub import create_branch, create_tag # Create a branch on a Space repo from `main` branch >>> create_branch("Matthijs/speecht5-tts-demo", repo_type="space", branch="handle-dog-speaker") # Create a tag on a Dataset repo from `v0.1-release` branch >>> create_tag("bigcode/the-stack", repo_type="dataset", revision="v0.1-release", tag="v0.1.1", tag_message="Bump release version.") You can use the delete_branch() and delete_tag() functions in the same way to delete a branch or a tag. List all branches and tags You can also list the existing git refs from a repository using list_repo_refs(): Copied >>> from huggingface_hub import list_repo_refs >>> list_repo_refs("bigcode/the-stack", repo_type="dataset") GitRefs( branches=[ GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'), GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714') ], converts=[], tags=[ GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da') ] ) Change repository settings Repositories come with some settings that you can configure. Most of the time, you will want to do that manually in the repo settings page in your browser. You must have write access to a repo to configure it (either own it or being part of an organization). In this section, we will see the settings that you can also configure programmatically using huggingface_hub. Some settings are specific to Spaces (hardware, environment variables,‚Ä¶). To configure those, please refer to our Manage your Spaces guide. Update visibility A repository can be public or private. A private repository is only visible to you or members of the organization in which the repository is located. Change a repository to private as shown in the following: Copied >>> from huggingface_hub import update_repo_visibility >>> update_repo_visibility(repo_id=repo_id, private=True) Rename your repository You can rename your repository on the Hub using move_repo(). Using this method, you can also move the repo from a user to an organization. When doing so, there are a few limitations that you should be aware of. For example, you can‚Äôt transfer your repo to another user. Copied >>> from huggingface_hub import move_repo >>> move_repo(from_id="Wauplin/cool-model", to_id="huggingface/cool-model") Manage a local copy of your repository All the actions described above can be done using HTTP requests. However, in some cases you might be interested in having a local copy of your repository and interact with it using the Git commands you are familiar with. The Repository class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see here for installation instructions). Repository is deprecated in favor of the http-based alternatives implemented in HfApi. Given its large adoption in legacy code, the complete removal of Repository will only happen in release v1.0. For more details, please read this explanation page. Use a local repository Instantiate a Repository object with a path to a local repository: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="<path>/<to>/<folder>") Clone The clone_from parameter clones a repository from a Hugging Face repository ID to a local directory specified by the local_dir argument: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="w2v2", clone_from="facebook/wav2vec2-large-960h-lv60") clone_from can also clone a repository using a URL: Copied >>> repo = Repository(local_dir="huggingface-hub", clone_from="https://huggingface.co/facebook/wav2vec2-large-960h-lv60") You can combine the clone_from parameter with create_repo() to create and clone a repository: Copied >>> repo_url = create_repo(repo_id="repo_name") >>> repo = Repository(local_dir="repo_local_path", clone_from=repo_url) You can also configure a Git username and email to a cloned repository by specifying the git_user and git_email parameters when you clone a repository. When users commit to that repository, Git will be aware of the commit author. Copied >>> repo = Repository( ... "my-dataset", ... clone_from="<user>/<dataset_id>", ... token=True, ... repo_type="dataset", ... git_user="MyName", ... git_email="me@cool.mail" ... ) Branch Branches are important for collaboration and experimentation without impacting your current files and code. Switch between branches with git_checkout(). For example, if you want to switch from branch1 to branch2: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="huggingface-hub", clone_from="<user>/<dataset_id>", revision='branch1') >>> repo.git_checkout("branch2") Pull git_pull() allows you to update a current local branch with changes from a remote repository: Copied >>> from huggingface_hub import Repository >>> repo.git_pull() Set rebase=True if you want your local commits to occur after your branch is updated with the new commits from the remote: Copied >>> repo.git_pull(rebase=True) < > Update on GitHub ‚ÜêHfFileSystem Search‚Üí Create and manage a repository Repo creation and deletion Create a repository Delete a repository Duplicate a repository (only for Spaces) Upload and download files Branches and tags Create branches and tags List all branches and tags Change repository settings Update visibility Rename your repository Manage a local copy of your repository Use a local repository Clone Branch Pull
Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Search the Hub Hub Python Library üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.23.3v0.22.2v0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Types Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Serialization Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Search the Hub In this tutorial, you will learn how to search models, datasets and spaces on the Hub using huggingface_hub. How to list repositories ? huggingface_hub library includes an HTTP client HfApi to interact with the Hub. Among other things, it can list models, datasets and spaces stored on the Hub: Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> models = api.list_models() The output of list_models() is an iterator over the models stored on the Hub. Similarly, you can use list_datasets() to list datasets and list_spaces() to list Spaces. How to filter repositories ? Listing repositories is great but now you might want to filter your search. The list helpers have several attributes like: filter author search ‚Ä¶ Two of these parameters are intuitive (author and search), but what about that filter? filter takes as input a ModelFilter object (or DatasetFilter). You can instantiate it by specifying which models you want to filter. Let‚Äôs see an example to get all models on the Hub that does image classification, have been trained on the imagenet dataset and that runs with PyTorch. That can be done with a single ModelFilter. Attributes are combined as ‚Äúlogical AND‚Äù. Copied models = hf_api.list_models( filter=ModelFilter( task="image-classification", library="pytorch", trained_dataset="imagenet" ) ) While filtering, you can also sort the models and take only the top results. For example, the following example fetches the top 5 most downloaded datasets on the Hub: Copied >>> list(list_datasets(sort="downloads", direction=-1, limit=5)) [DatasetInfo( id='argilla/databricks-dolly-15k-curated-en', author='argilla', sha='4dcd1dedbe148307a833c931b21ca456a1fc4281', last_modified=datetime.datetime(2023, 10, 2, 12, 32, 53, tzinfo=datetime.timezone.utc), private=False, downloads=8889377, (...) To explore available filter on the Hub, visit models and datasets pages in your browser, search for some parameters and look at the values in the URL. < > Update on GitHub ‚ÜêRepository Inference‚Üí Search the Hub How to list repositories ? How to filter repositories ?
